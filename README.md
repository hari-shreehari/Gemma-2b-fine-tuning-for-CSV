# Fine-Tuning Gemma 2B Model

This project demonstrates the fine-tuning of the Gemma 2B model using a CSV dataset of 100 multiple-choice questions and answers. The fine-tuning is performed using LoRA and Keras, and the process is executed on an Ubuntu machine without CUDA support.


## Introduction

This project focuses on fine-tuning the Gemma 2B model, a language model variant with 2 billion parameters. The fine-tuning is done using a CSV dataset containing 100 multiple-choice questions and answers. The project utilizes LoRA (Low-Rank Adaptation) and Keras for the fine-tuning process.


## Prerequisites

- Ubuntu operating system
- Python 3.8 or later
- No CUDA support required


## Dataset

The dataset used for fine-tuning consists of 100 multiple-choice questions and answers, stored in a CSV file. The structure of the CSV file is as per example uploaded.
